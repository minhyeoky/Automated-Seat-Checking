{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중심(x,y),넓이(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# BDML server GPU config\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "RCNN_DIR = os.path.join(ROOT_DIR, 'Mask_RCNN')\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(RCNN_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "# Import COCO config\n",
    "sys.path.append(os.path.join(RCNN_DIR, \"samples/coco/\"))  # To find local version\n",
    "import coco\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(RCNN_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(RCNN_DIR, \"mask_rcnn_coco.h5\")\n",
    "\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = os.path.join(ROOT_DIR, \"imgs/clean_2_eval_augmented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        200\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                93\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           coco\n",
      "NUM_CLASSES                    81\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(coco.CocoConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    DETECTION_MIN_CONFIDENCE = 0.7 # 0.9->0.7 로 조정\n",
    "    DETECTION_MAX_INSTANCES = 200\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model and Load Trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mhlee/miniconda3/envs/maskrcnn/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n"
     ]
    }
   ],
   "source": [
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO Class names\n",
    "# Index of the class in the list is its ID. For example, to get ID of\n",
    "# the teddy bear class, use: class_names.index('teddy bear')\n",
    "class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "               'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
    "               'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
    "               'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "               'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "               'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
    "               'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "               'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
    "               'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "               'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
    "               'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
    "               'teddy bear', 'hair drier', 'toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 class_229_r_+_9.jpg\n"
     ]
    }
   ],
   "source": [
    "# Load a random image from the images folder\n",
    "file_names = next(os.walk(IMAGE_DIR))[2]\n",
    "print(len(file_names), file_names[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACTE FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_feature(r):\n",
    "    class_ids = r['class_ids']\n",
    "    masks = r['masks']\n",
    "    rois = r['rois']\n",
    "    input_features = []\n",
    "    persons_masks = np.zeros_like(masks[:,:,0])\n",
    "    for i in range(len(class_ids)):\n",
    "\n",
    "        if class_ids[i] == 1: # 사람 선택,\n",
    "            target = masks[:,:,i]\n",
    "            box = rois[i]\n",
    "            c1, c2 = ndimage.center_of_mass(target, target)\n",
    "            center = c1 - 512, c2 - 512\n",
    "            y1, x1, y2, x2 = box[0], box[1], box[2], box[3]\n",
    "            area = ((x2 - x1) * (y2 - y1))/1024\n",
    "#             print(area, center, class_names[class_ids[i]])\n",
    "            input_feature = np.concatenate((center, area.reshape((1))))\n",
    "            input_features.append(input_feature)\n",
    "#             print(input_feature)\n",
    "            \n",
    "    max_input_features = 8*3*3\n",
    "    if input_features == []:\n",
    "        input_features = [[0],[0]]\n",
    "    random.shuffle(input_features)\n",
    "    input_features = np.concatenate(input_features) #하나로\n",
    "    \n",
    "    # PADDING\n",
    "\n",
    "    while(True):\n",
    "        if len(input_features)  >= max_input_features:\n",
    "            input_features = input_features[:max_input_features] \n",
    "            break\n",
    "        \n",
    "        input_features = np.append(input_features, 0)\n",
    "    return input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file_name(fn):\n",
    "#     print(fn)\n",
    "    fn_orig = fn\n",
    "    fn = fn.split('.')\n",
    "    fn = fn[0].split('_')\n",
    "    fn.append('dummy') # 4.jpg\n",
    "    label = int(fn[1]) # class_lable_....\n",
    "    reverse = False\n",
    "    if 'r' in fn_orig:\n",
    "        reverse = True\n",
    "#     print(reverse)\n",
    "    return label, reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_person_mask(results, show=True):\n",
    "    output = []\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    for i in range(12):\n",
    "        person_mask = np.zeros_like(results[0]['masks'][:,:,0])\n",
    "        r = results[i]\n",
    "        ax = plt.subplot(4,3,i+1)\n",
    "\n",
    "        for j in range(r['masks'].shape[2]):\n",
    "            height = r['masks'][:,:,j].shape[0]\n",
    "            width = r['masks'][:,:,j].shape[1]\n",
    "\n",
    "            if r['class_ids'][j] == 1:\n",
    "                person_mask = person_mask + r['masks'][:,:,j]\n",
    "        output.append(person_mask)\n",
    "        person_mask = skimage.transform.resize(person_mask, (128,128))\n",
    "        ax.imshow(person_mask)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFDataset serialize functions\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    " \n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def create_tf_example(input_feature, input_label, input_file_name):\n",
    "    \n",
    "        input_file_name = input_file_name.encode()\n",
    "        input_feature = np.array(input_feature).tostring()\n",
    "        input_label = np.array(input_label).tostring()\n",
    "        features = {\n",
    "            'file_name':_bytes_feature(input_file_name),\n",
    "            'feature':_bytes_feature(input_feature),\n",
    "            'label':_bytes_feature(input_label)\n",
    "        }\n",
    "        features = tf.train.Features(feature=features)\n",
    "        example = tf.train.Example(features=features)\n",
    "        return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../label/clean_2_label.pickle', mode='rb') as f:\n",
    "    labels = np.array(pickle.load(f))\n",
    "with open('../label/clean_2_label_r.pickle', mode='rb') as f:\n",
    "    labels_r = np.array(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373ff467957c4a9980de9014cf63f24e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=126), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DIR_PATH = '../input_feature'\n",
    "TFRecord_file = 'clean_3_eval.TFRecord'\n",
    "\n",
    "batches = []\n",
    "input_features = []\n",
    "input_labels = []\n",
    "input_file_names = []\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "with tf.python_io.TFRecordWriter(os.path.join(DIR_PATH, TFRecord_file)) as writer:\n",
    "    for file_name in tqdm_notebook(file_names):\n",
    "        input_file_names.append(file_name)\n",
    "\n",
    "        label, reverse = parse_file_name(file_name)\n",
    "        if reverse:\n",
    "            input_labels.append(labels_r[label])\n",
    "        else:\n",
    "            input_labels.append(labels[label])\n",
    "\n",
    "        image = skimage.io.imread(os.path.join(IMAGE_DIR, file_name))\n",
    "        batches.append(image)\n",
    "\n",
    "        if len(batches) == BATCH_SIZE:\n",
    "            results = model.detect(batches, verbose=0)\n",
    "    #         create_person_mask(results)\n",
    "            for i in range(len(results)):\n",
    "                input_feature = create_input_feature(results[i])\n",
    "                input_features.append(input_feature)\n",
    "    #         visualize.display_instances(batches[0], results[0]['rois'], results[0]['masks'], \n",
    "    #                                  results[0]['class_ids'], class_names, results[0]['scores'])\n",
    "    #         print(input_features[0])\n",
    "    #         print(input_labels[0])\n",
    "    #         print(input_file_names[0])\n",
    "#             visualize.display_instances(batches[0], results[0]['rois'], \n",
    "#                                         results[0]['masks'], \n",
    "#                                         results[0]['class_ids'], \n",
    "#                                         class_names)\n",
    "            assert len(input_features) == BATCH_SIZE\n",
    "            assert len(input_labels) == BATCH_SIZE\n",
    "            assert len(input_file_names) == BATCH_SIZE\n",
    "\n",
    "            for i in range(BATCH_SIZE):\n",
    "                writer.write(create_tf_example(input_features[i],\n",
    "                                               input_labels[i],\n",
    "                                               input_file_names[i]).SerializeToString())\n",
    "            input_features = []\n",
    "            input_labels = []\n",
    "            input_file_names = []\n",
    "            batches = []\n",
    "\n",
    "\n",
    "                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
